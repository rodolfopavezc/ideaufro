{"ast":null,"code":"var _s = $RefreshSig$();\n\nimport { useState, useEffect, useRef } from 'react';\nimport Hark from 'hark';\nimport { startRecording, stopRecording } from './recorderHelpers'; // https://cloud.google.com/speech-to-text/docs/reference/rest/v1/RecognitionConfig\n\nconst isEdgeChromium = navigator.userAgent.indexOf('Edg/') !== -1;\nconst AudioContext = window.AudioContext || window.webkitAudioContext;\nconst SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\nlet recognition;\n\n// Set recognition back to null for brave browser due to promise resolving\n// after the conditional on line 31\nif (navigator.brave) {\n  navigator.brave.isBrave().then(bool => {\n    if (bool) recognition = null;\n  });\n} // Chromium browsers will have the SpeechRecognition method\n// but do not implement the functionality due to google wanting ðŸ’°\n// this covers new Edge and line 22 covers Brave, the two most popular non-chrome chromium browsers\n\n\nif (!isEdgeChromium && SpeechRecognition) {\n  recognition = new SpeechRecognition();\n}\n\nexport default function useSpeechToText({\n  continuous,\n  crossBrowser,\n  googleApiKey,\n  googleCloudRecognitionConfig,\n  onStartSpeaking,\n  onStoppedSpeaking,\n  speechRecognitionProperties = {\n    interimResults: true\n  },\n  timeout = 10000,\n  useOnlyGoogleCloud = false,\n  useLegacyResults = true\n}) {\n  _s();\n\n  const [isRecording, setIsRecording] = useState(false);\n  const audioContextRef = useRef();\n  const [legacyResults, setLegacyResults] = useState([]);\n  const [results, setResults] = useState([]);\n  const [sentence, setSentence] = useState('');\n  const [interimResult, setInterimResult] = useState();\n  const [error, setError] = useState('');\n  const timeoutId = useRef();\n  const mediaStream = useRef();\n  useEffect(() => {\n    var _navigator, _navigator$mediaDevic;\n\n    if (!crossBrowser && !recognition) {\n      setError('Speech Recognition API is only available on Chrome');\n    }\n\n    if (!((_navigator = navigator) === null || _navigator === void 0 ? void 0 : (_navigator$mediaDevic = _navigator.mediaDevices) === null || _navigator$mediaDevic === void 0 ? void 0 : _navigator$mediaDevic.getUserMedia)) {\n      setError('getUserMedia is not supported on this device/browser :(');\n    }\n\n    if ((crossBrowser || useOnlyGoogleCloud) && !googleApiKey) {\n      console.error('No google cloud API key was passed, google API will not be able to process speech');\n    }\n\n    if (!audioContextRef.current) {\n      audioContextRef.current = new AudioContext();\n    }\n\n    if (useLegacyResults) {\n      console.warn('react-hook-speech-to-text is using legacy results, pass useLegacyResults: false to the hook to use the new array of objects results. Legacy array of strings results will be removed in a future version.');\n    }\n  }, []); // Chrome Speech Recognition API:\n  // Only supported on Chrome browsers\n\n  const chromeSpeechRecognition = () => {\n    if (recognition) {\n      // Continuous recording after stopped speaking event\n      if (continuous) recognition.continuous = true;\n      const {\n        grammars,\n        interimResults,\n        lang,\n        maxAlternatives\n      } = speechRecognitionProperties || {};\n      if (grammars) recognition.grammars = grammars;\n      if (lang) recognition.lang = lang;\n      recognition.interimResults = interimResults || false;\n      recognition.maxAlternatives = maxAlternatives || 1; // start recognition\n\n      recognition.start(); // speech successfully translated into text\n\n      recognition.onresult = e => {\n        const result = e.results[e.results.length - 1];\n        const {\n          transcript\n        } = result[0];\n        const timestamp = Math.floor(Date.now() / 1000); // Allows for realtime speech result UI feedback\n\n        if (interimResults) {\n          if (result.isFinal) {\n            setInterimResult(undefined);\n            setResults(prevResults => [...prevResults, {\n              transcript,\n              timestamp\n            }]);\n            setLegacyResults(prevResults => [...prevResults, transcript]);\n            setSentence(transcript);\n            analizarMensaje(transcript);\n          } else {\n            let concatTranscripts = ''; // If continuous: e.results will include previous speech results: need to start loop at the current event resultIndex for proper concatenation\n\n            for (let i = e.resultIndex; i < e.results.length; i++) {\n              concatTranscripts += e.results[i][0].transcript;\n            }\n\n            setInterimResult(concatTranscripts);\n          }\n        } else {\n          setResults(prevResults => [...prevResults, {\n            transcript,\n            timestamp\n          }]);\n          setLegacyResults(prevResults => [...prevResults, transcript]);\n        }\n      };\n\n      recognition.onaudiostart = () => setIsRecording(true); // Audio stopped recording or timed out.\n      // Chrome speech auto times-out if no speech after a while\n\n\n      recognition.onend = () => {\n        setIsRecording(false);\n      };\n    }\n  };\n\n  const startSpeechToText = async () => {\n    var _audioContextRef$curr;\n\n    if (!useOnlyGoogleCloud && recognition) {\n      chromeSpeechRecognition();\n      return;\n    }\n\n    if (!crossBrowser && !useOnlyGoogleCloud) {\n      return;\n    } // Resume audio context due to google auto play policy\n    // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio\n\n\n    if (((_audioContextRef$curr = audioContextRef.current) === null || _audioContextRef$curr === void 0 ? void 0 : _audioContextRef$curr.state) === 'suspended') {\n      var _audioContextRef$curr2;\n\n      (_audioContextRef$curr2 = audioContextRef.current) === null || _audioContextRef$curr2 === void 0 ? void 0 : _audioContextRef$curr2.resume();\n    }\n\n    const stream = await startRecording({\n      errHandler: () => setError('Microphone permission was denied'),\n      audioContext: audioContextRef.current\n    });\n    setIsRecording(true); // Stop recording if timeout\n\n    if (timeout) {\n      clearTimeout(timeoutId.current);\n      handleRecordingTimeout();\n    } // stop previous mediaStream track if exists\n\n\n    if (mediaStream.current) {\n      stopMediaStream();\n    } // Clones stream to fix hark bug on Safari\n\n\n    mediaStream.current = stream.clone();\n    const speechEvents = Hark(mediaStream.current, {\n      audioContext: audioContextRef.current\n    });\n    speechEvents.on('speaking', () => {\n      if (onStartSpeaking) onStartSpeaking(); // Clear previous recording timeout on every speech event\n\n      clearTimeout(timeoutId.current);\n    });\n    speechEvents.on('stopped_speaking', () => {\n      if (onStoppedSpeaking) onStoppedSpeaking(); // Stops current recording and sends audio string to google cloud.\n      // recording will start again after google cloud api\n      // call if `continuous` prop is true. Until the api result\n      // returns, technically the microphone is not being captured again\n\n      stopRecording({\n        exportWAV: true,\n        wavCallback: blob => handleBlobToBase64({\n          blob,\n          continuous: continuous || false\n        })\n      });\n    });\n  };\n\n  const stopSpeechToText = () => {\n    if (recognition && !useOnlyGoogleCloud) {\n      recognition.stop();\n    } else {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({\n        exportWAV: true,\n        wavCallback: blob => handleBlobToBase64({\n          blob,\n          continuous: false\n        })\n      });\n    }\n  };\n\n  const handleRecordingTimeout = () => {\n    timeoutId.current = window.setTimeout(() => {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({\n        exportWAV: false\n      });\n    }, timeout);\n  };\n\n  const handleBlobToBase64 = ({\n    blob,\n    continuous\n  }) => {\n    const reader = new FileReader();\n    reader.readAsDataURL(blob);\n\n    reader.onloadend = async () => {\n      var _audioContextRef$curr3, _googleCloudJson$resu;\n\n      const base64data = reader.result;\n      let sampleRate = (_audioContextRef$curr3 = audioContextRef.current) === null || _audioContextRef$curr3 === void 0 ? void 0 : _audioContextRef$curr3.sampleRate; // Google only accepts max 48000 sample rate: if\n      // greater recorder js will down-sample to 48000\n\n      if (sampleRate && sampleRate > 48000) {\n        sampleRate = 48000;\n      }\n\n      const audio = {\n        content: ''\n      };\n      const config = {\n        encoding: 'LINEAR16',\n        languageCode: 'es-ES',\n        sampleRateHertz: sampleRate,\n        ...googleCloudRecognitionConfig\n      };\n      const data = {\n        config,\n        audio\n      }; // Gets raw base 64 string data\n\n      audio.content = base64data.substr(base64data.indexOf(',') + 1);\n      const googleCloudRes = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=${googleApiKey}`, {\n        method: 'POST',\n        body: JSON.stringify(data)\n      });\n      const googleCloudJson = await googleCloudRes.json(); // Update results state with transcribed text\n\n      if (((_googleCloudJson$resu = googleCloudJson.results) === null || _googleCloudJson$resu === void 0 ? void 0 : _googleCloudJson$resu.length) > 0) {\n        const {\n          transcript\n        } = googleCloudJson.results[0].alternatives[0];\n        setLegacyResults(prevResults => [...prevResults, transcript]);\n        setResults(prevResults => [...prevResults, {\n          speechBlob: blob,\n          transcript,\n          timestamp: Math.floor(Date.now() / 1000)\n        }]);\n      }\n\n      if (continuous) {\n        startSpeechToText();\n      } else {\n        stopMediaStream();\n        setIsRecording(false);\n      }\n    };\n  };\n\n  const stopMediaStream = () => {\n    var _mediaStream$current;\n\n    (_mediaStream$current = mediaStream.current) === null || _mediaStream$current === void 0 ? void 0 : _mediaStream$current.getAudioTracks()[0].stop();\n  };\n\n  const analizarMensaje = mensaje => {//Consultar a dialogFlow\n  };\n\n  return {\n    error,\n    interimResult,\n    isRecording,\n    sentence,\n    results: useLegacyResults ? legacyResults : results,\n    setResults,\n    startSpeechToText,\n    stopSpeechToText\n  };\n}\n\n_s(useSpeechToText, \"yWpRylv4l4no9OIRJreoN2UeWs8=\");","map":{"version":3,"sources":["/Users/rodolfopavez/Desktop/ideaufro/frontend/src/Hooks/index.tsx"],"names":["useState","useEffect","useRef","Hark","startRecording","stopRecording","isEdgeChromium","navigator","userAgent","indexOf","AudioContext","window","webkitAudioContext","SpeechRecognition","webkitSpeechRecognition","recognition","brave","isBrave","then","bool","useSpeechToText","continuous","crossBrowser","googleApiKey","googleCloudRecognitionConfig","onStartSpeaking","onStoppedSpeaking","speechRecognitionProperties","interimResults","timeout","useOnlyGoogleCloud","useLegacyResults","isRecording","setIsRecording","audioContextRef","legacyResults","setLegacyResults","results","setResults","sentence","setSentence","interimResult","setInterimResult","error","setError","timeoutId","mediaStream","mediaDevices","getUserMedia","console","current","warn","chromeSpeechRecognition","grammars","lang","maxAlternatives","start","onresult","e","result","length","transcript","timestamp","Math","floor","Date","now","isFinal","undefined","prevResults","analizarMensaje","concatTranscripts","i","resultIndex","onaudiostart","onend","startSpeechToText","state","resume","stream","errHandler","audioContext","clearTimeout","handleRecordingTimeout","stopMediaStream","clone","speechEvents","on","exportWAV","wavCallback","blob","handleBlobToBase64","stopSpeechToText","stop","setTimeout","reader","FileReader","readAsDataURL","onloadend","base64data","sampleRate","audio","content","config","encoding","languageCode","sampleRateHertz","data","substr","googleCloudRes","fetch","method","body","JSON","stringify","googleCloudJson","json","alternatives","speechBlob","getAudioTracks","mensaje"],"mappings":";;AAAA,SAASA,QAAT,EAAmBC,SAAnB,EAA8BC,MAA9B,QAA4C,OAA5C;AACA,OAAOC,IAAP,MAAiB,MAAjB;AACA,SAASC,cAAT,EAAyBC,aAAzB,QAA8C,mBAA9C,C,CAEA;;AAYA,MAAMC,cAAc,GAAGC,SAAS,CAACC,SAAV,CAAoBC,OAApB,CAA4B,MAA5B,MAAwC,CAAC,CAAhE;AAQA,MAAMC,YAAY,GAAGC,MAAM,CAACD,YAAP,IAAwBC,MAAD,CAAgBC,kBAA5D;AAEA,MAAMC,iBAAiB,GACrBF,MAAM,CAACE,iBAAP,IAA6BF,MAAD,CAAgBG,uBAD9C;AAGA,IAAIC,WAAJ;;AAQA;AACA;AACA,IAAKR,SAAD,CAA8BS,KAAlC,EAAyC;AACtCT,EAAAA,SAAD,CAA8BS,KAA9B,CAAoCC,OAApC,GAA8CC,IAA9C,CAAoDC,IAAD,IAAU;AAC3D,QAAIA,IAAJ,EAAUJ,WAAW,GAAG,IAAd;AACX,GAFD;AAGD,C,CAED;AACA;AACA;;;AACA,IAAI,CAACT,cAAD,IAAmBO,iBAAvB,EAA0C;AACxCE,EAAAA,WAAW,GAAG,IAAIF,iBAAJ,EAAd;AACD;;AAeD,eAAe,SAASO,eAAT,CAAyB;AACtCC,EAAAA,UADsC;AAEtCC,EAAAA,YAFsC;AAGtCC,EAAAA,YAHsC;AAItCC,EAAAA,4BAJsC;AAKtCC,EAAAA,eALsC;AAMtCC,EAAAA,iBANsC;AAOtCC,EAAAA,2BAA2B,GAAG;AAAEC,IAAAA,cAAc,EAAE;AAAlB,GAPQ;AAQtCC,EAAAA,OAAO,GAAG,KAR4B;AAStCC,EAAAA,kBAAkB,GAAG,KATiB;AAUtCC,EAAAA,gBAAgB,GAAG;AAVmB,CAAzB,EAWU;AAAA;;AACvB,QAAM,CAACC,WAAD,EAAcC,cAAd,IAAgCjC,QAAQ,CAAC,KAAD,CAA9C;AAEA,QAAMkC,eAAe,GAAGhC,MAAM,EAA9B;AAEA,QAAM,CAACiC,aAAD,EAAgBC,gBAAhB,IAAoCpC,QAAQ,CAAW,EAAX,CAAlD;AACA,QAAM,CAACqC,OAAD,EAAUC,UAAV,IAAwBtC,QAAQ,CAAe,EAAf,CAAtC;AACA,QAAM,CAACuC,QAAD,EAAWC,WAAX,IAA0BxC,QAAQ,CAAC,EAAD,CAAxC;AAEA,QAAM,CAACyC,aAAD,EAAgBC,gBAAhB,IAAoC1C,QAAQ,EAAlD;AACA,QAAM,CAAC2C,KAAD,EAAQC,QAAR,IAAoB5C,QAAQ,CAAC,EAAD,CAAlC;AAEA,QAAM6C,SAAS,GAAG3C,MAAM,EAAxB;AACA,QAAM4C,WAAW,GAAG5C,MAAM,EAA1B;AAEAD,EAAAA,SAAS,CAAC,MAAM;AAAA;;AACd,QAAI,CAACqB,YAAD,IAAiB,CAACP,WAAtB,EAAmC;AACjC6B,MAAAA,QAAQ,CAAC,oDAAD,CAAR;AACD;;AAED,QAAI,gBAACrC,SAAD,wEAAC,WAAWwC,YAAZ,0DAAC,sBAAyBC,YAA1B,CAAJ,EAA4C;AAC1CJ,MAAAA,QAAQ,CAAC,yDAAD,CAAR;AACD;;AAED,QAAI,CAACtB,YAAY,IAAIQ,kBAAjB,KAAwC,CAACP,YAA7C,EAA2D;AACzD0B,MAAAA,OAAO,CAACN,KAAR,CACE,mFADF;AAGD;;AAED,QAAI,CAACT,eAAe,CAACgB,OAArB,EAA8B;AAC5BhB,MAAAA,eAAe,CAACgB,OAAhB,GAA0B,IAAIxC,YAAJ,EAA1B;AACD;;AAED,QAAIqB,gBAAJ,EAAsB;AACpBkB,MAAAA,OAAO,CAACE,IAAR,CACE,2MADF;AAGD;AACF,GAxBQ,EAwBN,EAxBM,CAAT,CAfuB,CAyCvB;AACA;;AACA,QAAMC,uBAAuB,GAAG,MAAM;AACpC,QAAIrC,WAAJ,EAAiB;AACf;AACA,UAAIM,UAAJ,EAAgBN,WAAW,CAACM,UAAZ,GAAyB,IAAzB;AAEhB,YAAM;AAAEgC,QAAAA,QAAF;AAAYzB,QAAAA,cAAZ;AAA4B0B,QAAAA,IAA5B;AAAkCC,QAAAA;AAAlC,UACJ5B,2BAA2B,IAAI,EADjC;AAGA,UAAI0B,QAAJ,EAActC,WAAW,CAACsC,QAAZ,GAAuBA,QAAvB;AACd,UAAIC,IAAJ,EAAUvC,WAAW,CAACuC,IAAZ,GAAmBA,IAAnB;AAEVvC,MAAAA,WAAW,CAACa,cAAZ,GAA6BA,cAAc,IAAI,KAA/C;AACAb,MAAAA,WAAW,CAACwC,eAAZ,GAA8BA,eAAe,IAAI,CAAjD,CAXe,CAaf;;AACAxC,MAAAA,WAAW,CAACyC,KAAZ,GAde,CAgBf;;AACAzC,MAAAA,WAAW,CAAC0C,QAAZ,GAAwBC,CAAD,IAAO;AAC5B,cAAMC,MAAM,GAAGD,CAAC,CAACrB,OAAF,CAAUqB,CAAC,CAACrB,OAAF,CAAUuB,MAAV,GAAmB,CAA7B,CAAf;AACA,cAAM;AAAEC,UAAAA;AAAF,YAAiBF,MAAM,CAAC,CAAD,CAA7B;AACA,cAAMG,SAAS,GAAGC,IAAI,CAACC,KAAL,CAAWC,IAAI,CAACC,GAAL,KAAa,IAAxB,CAAlB,CAH4B,CAK5B;;AACA,YAAItC,cAAJ,EAAoB;AAClB,cAAI+B,MAAM,CAACQ,OAAX,EAAoB;AAClBzB,YAAAA,gBAAgB,CAAC0B,SAAD,CAAhB;AACA9B,YAAAA,UAAU,CAAE+B,WAAD,IAAiB,CAC1B,GAAGA,WADuB,EAE1B;AAAER,cAAAA,UAAF;AAAcC,cAAAA;AAAd,aAF0B,CAAlB,CAAV;AAIA1B,YAAAA,gBAAgB,CAAEiC,WAAD,IAAiB,CAAC,GAAGA,WAAJ,EAAiBR,UAAjB,CAAlB,CAAhB;AACArB,YAAAA,WAAW,CAACqB,UAAD,CAAX;AACAS,YAAAA,eAAe,CAACT,UAAD,CAAf;AACD,WATD,MASO;AACL,gBAAIU,iBAAiB,GAAG,EAAxB,CADK,CAGL;;AACA,iBAAK,IAAIC,CAAC,GAAGd,CAAC,CAACe,WAAf,EAA4BD,CAAC,GAAGd,CAAC,CAACrB,OAAF,CAAUuB,MAA1C,EAAkDY,CAAC,EAAnD,EAAuD;AACrDD,cAAAA,iBAAiB,IAAIb,CAAC,CAACrB,OAAF,CAAUmC,CAAV,EAAa,CAAb,EAAgBX,UAArC;AACD;;AAEDnB,YAAAA,gBAAgB,CAAC6B,iBAAD,CAAhB;AACD;AACF,SApBD,MAoBO;AACLjC,UAAAA,UAAU,CAAE+B,WAAD,IAAiB,CAC1B,GAAGA,WADuB,EAE1B;AAAER,YAAAA,UAAF;AAAcC,YAAAA;AAAd,WAF0B,CAAlB,CAAV;AAIA1B,UAAAA,gBAAgB,CAAEiC,WAAD,IAAiB,CAAC,GAAGA,WAAJ,EAAiBR,UAAjB,CAAlB,CAAhB;AACD;AACF,OAjCD;;AAmCA9C,MAAAA,WAAW,CAAC2D,YAAZ,GAA2B,MAAMzC,cAAc,CAAC,IAAD,CAA/C,CApDe,CAsDf;AACA;;;AACAlB,MAAAA,WAAW,CAAC4D,KAAZ,GAAoB,MAAM;AACxB1C,QAAAA,cAAc,CAAC,KAAD,CAAd;AACD,OAFD;AAGD;AACF,GA7DD;;AA+DA,QAAM2C,iBAAiB,GAAG,YAAY;AAAA;;AACpC,QAAI,CAAC9C,kBAAD,IAAuBf,WAA3B,EAAwC;AACtCqC,MAAAA,uBAAuB;AACvB;AACD;;AAED,QAAI,CAAC9B,YAAD,IAAiB,CAACQ,kBAAtB,EAA0C;AACxC;AACD,KARmC,CAUpC;AACA;;;AACA,QAAI,0BAAAI,eAAe,CAACgB,OAAhB,gFAAyB2B,KAAzB,MAAmC,WAAvC,EAAoD;AAAA;;AAClD,gCAAA3C,eAAe,CAACgB,OAAhB,kFAAyB4B,MAAzB;AACD;;AAED,UAAMC,MAAM,GAAG,MAAM3E,cAAc,CAAC;AAClC4E,MAAAA,UAAU,EAAE,MAAMpC,QAAQ,CAAC,kCAAD,CADQ;AAElCqC,MAAAA,YAAY,EAAE/C,eAAe,CAACgB;AAFI,KAAD,CAAnC;AAKAjB,IAAAA,cAAc,CAAC,IAAD,CAAd,CArBoC,CAuBpC;;AACA,QAAIJ,OAAJ,EAAa;AACXqD,MAAAA,YAAY,CAACrC,SAAS,CAACK,OAAX,CAAZ;AACAiC,MAAAA,sBAAsB;AACvB,KA3BmC,CA6BpC;;;AACA,QAAIrC,WAAW,CAACI,OAAhB,EAAyB;AACvBkC,MAAAA,eAAe;AAChB,KAhCmC,CAkCpC;;;AACAtC,IAAAA,WAAW,CAACI,OAAZ,GAAsB6B,MAAM,CAACM,KAAP,EAAtB;AAEA,UAAMC,YAAY,GAAGnF,IAAI,CAAC2C,WAAW,CAACI,OAAb,EAAsB;AAC7C+B,MAAAA,YAAY,EAAE/C,eAAe,CAACgB;AADe,KAAtB,CAAzB;AAIAoC,IAAAA,YAAY,CAACC,EAAb,CAAgB,UAAhB,EAA4B,MAAM;AAChC,UAAI9D,eAAJ,EAAqBA,eAAe,GADJ,CAGhC;;AACAyD,MAAAA,YAAY,CAACrC,SAAS,CAACK,OAAX,CAAZ;AACD,KALD;AAOAoC,IAAAA,YAAY,CAACC,EAAb,CAAgB,kBAAhB,EAAoC,MAAM;AACxC,UAAI7D,iBAAJ,EAAuBA,iBAAiB,GADA,CAGxC;AACA;AACA;AACA;;AACArB,MAAAA,aAAa,CAAC;AACZmF,QAAAA,SAAS,EAAE,IADC;AAEZC,QAAAA,WAAW,EAAGC,IAAD,IACXC,kBAAkB,CAAC;AAAED,UAAAA,IAAF;AAAQrE,UAAAA,UAAU,EAAEA,UAAU,IAAI;AAAlC,SAAD;AAHR,OAAD,CAAb;AAKD,KAZD;AAaD,GA7DD;;AA+DA,QAAMuE,gBAAgB,GAAG,MAAM;AAC7B,QAAI7E,WAAW,IAAI,CAACe,kBAApB,EAAwC;AACtCf,MAAAA,WAAW,CAAC8E,IAAZ;AACD,KAFD,MAEO;AACL5D,MAAAA,cAAc,CAAC,KAAD,CAAd;AACAmD,MAAAA,eAAe;AACf/E,MAAAA,aAAa,CAAC;AACZmF,QAAAA,SAAS,EAAE,IADC;AAEZC,QAAAA,WAAW,EAAGC,IAAD,IAAUC,kBAAkB,CAAC;AAAED,UAAAA,IAAF;AAAQrE,UAAAA,UAAU,EAAE;AAApB,SAAD;AAF7B,OAAD,CAAb;AAID;AACF,GAXD;;AAaA,QAAM8D,sBAAsB,GAAG,MAAM;AACnCtC,IAAAA,SAAS,CAACK,OAAV,GAAoBvC,MAAM,CAACmF,UAAP,CAAkB,MAAM;AAC1C7D,MAAAA,cAAc,CAAC,KAAD,CAAd;AACAmD,MAAAA,eAAe;AACf/E,MAAAA,aAAa,CAAC;AAAEmF,QAAAA,SAAS,EAAE;AAAb,OAAD,CAAb;AACD,KAJmB,EAIjB3D,OAJiB,CAApB;AAKD,GAND;;AAQA,QAAM8D,kBAAkB,GAAG,CAAC;AAC1BD,IAAAA,IAD0B;AAE1BrE,IAAAA;AAF0B,GAAD,KAMrB;AACJ,UAAM0E,MAAM,GAAG,IAAIC,UAAJ,EAAf;AACAD,IAAAA,MAAM,CAACE,aAAP,CAAqBP,IAArB;;AAEAK,IAAAA,MAAM,CAACG,SAAP,GAAmB,YAAY;AAAA;;AAC7B,YAAMC,UAAU,GAAGJ,MAAM,CAACpC,MAA1B;AAEA,UAAIyC,UAAU,6BAAGlE,eAAe,CAACgB,OAAnB,2DAAG,uBAAyBkD,UAA1C,CAH6B,CAK7B;AACA;;AACA,UAAIA,UAAU,IAAIA,UAAU,GAAG,KAA/B,EAAsC;AACpCA,QAAAA,UAAU,GAAG,KAAb;AACD;;AAED,YAAMC,KAAK,GAAG;AAAEC,QAAAA,OAAO,EAAE;AAAX,OAAd;AAEA,YAAMC,MAAoC,GAAG;AAC3CC,QAAAA,QAAQ,EAAE,UADiC;AAE3CC,QAAAA,YAAY,EAAE,OAF6B;AAG3CC,QAAAA,eAAe,EAAEN,UAH0B;AAI3C,WAAG5E;AAJwC,OAA7C;AAOA,YAAMmF,IAAI,GAAG;AACXJ,QAAAA,MADW;AAEXF,QAAAA;AAFW,OAAb,CApB6B,CAyB7B;;AACAA,MAAAA,KAAK,CAACC,OAAN,GAAgBH,UAAU,CAACS,MAAX,CAAkBT,UAAU,CAAC1F,OAAX,CAAmB,GAAnB,IAA0B,CAA5C,CAAhB;AAEA,YAAMoG,cAAc,GAAG,MAAMC,KAAK,CAC/B,yDAAwDvF,YAAa,EADtC,EAEhC;AACEwF,QAAAA,MAAM,EAAE,MADV;AAEEC,QAAAA,IAAI,EAAEC,IAAI,CAACC,SAAL,CAAeP,IAAf;AAFR,OAFgC,CAAlC;AAQA,YAAMQ,eAAe,GAAG,MAAMN,cAAc,CAACO,IAAf,EAA9B,CApC6B,CAsC7B;;AACA,UAAI,0BAAAD,eAAe,CAAC9E,OAAhB,gFAAyBuB,MAAzB,IAAkC,CAAtC,EAAyC;AACvC,cAAM;AAAEC,UAAAA;AAAF,YAAiBsD,eAAe,CAAC9E,OAAhB,CAAwB,CAAxB,EAA2BgF,YAA3B,CAAwC,CAAxC,CAAvB;AACAjF,QAAAA,gBAAgB,CAAEiC,WAAD,IAAiB,CAAC,GAAGA,WAAJ,EAAiBR,UAAjB,CAAlB,CAAhB;AAEAvB,QAAAA,UAAU,CAAE+B,WAAD,IAAiB,CAC1B,GAAGA,WADuB,EAE1B;AACEiD,UAAAA,UAAU,EAAE5B,IADd;AAEE7B,UAAAA,UAFF;AAGEC,UAAAA,SAAS,EAAEC,IAAI,CAACC,KAAL,CAAWC,IAAI,CAACC,GAAL,KAAa,IAAxB;AAHb,SAF0B,CAAlB,CAAV;AAQD;;AAED,UAAI7C,UAAJ,EAAgB;AACduD,QAAAA,iBAAiB;AAClB,OAFD,MAEO;AACLQ,QAAAA,eAAe;AACfnD,QAAAA,cAAc,CAAC,KAAD,CAAd;AACD;AACF,KA3DD;AA4DD,GAtED;;AAwEA,QAAMmD,eAAe,GAAG,MAAM;AAAA;;AAC5B,4BAAAtC,WAAW,CAACI,OAAZ,8EAAqBqE,cAArB,GAAsC,CAAtC,EAAyC1B,IAAzC;AACD,GAFD;;AAIA,QAAMvB,eAAe,GAAIkD,OAAD,IAAY,CAChC;AAEH,GAHD;;AAKA,SAAO;AACL7E,IAAAA,KADK;AAELF,IAAAA,aAFK;AAGLT,IAAAA,WAHK;AAILO,IAAAA,QAJK;AAKLF,IAAAA,OAAO,EAAEN,gBAAgB,GAAGI,aAAH,GAAmBE,OALvC;AAMLC,IAAAA,UANK;AAOLsC,IAAAA,iBAPK;AAQLgB,IAAAA;AARK,GAAP;AAUD;;GApSuBxE,e","sourcesContent":["import { useState, useEffect, useRef } from 'react';\nimport Hark from 'hark';\nimport { startRecording, stopRecording } from './recorderHelpers';\n\n// https://cloud.google.com/speech-to-text/docs/reference/rest/v1/RecognitionConfig\nimport { GoogleCloudRecognitionConfig } from './GoogleCloudRecognitionConfig';\n\n// https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition\nexport interface SpeechRecognitionProperties {\n  // continuous: do not pass continuous here, instead pass it as a param to the hook\n  grammars?: SpeechGrammarList;\n  interimResults?: boolean;\n  lang?: string;\n  maxAlternatives?: number;\n}\n\nconst isEdgeChromium = navigator.userAgent.indexOf('Edg/') !== -1;\n\ninterface BraveNavigator extends Navigator {\n  brave: {\n    isBrave: () => Promise<boolean>;\n  };\n}\n\nconst AudioContext = window.AudioContext || (window as any).webkitAudioContext;\n\nconst SpeechRecognition =\n  window.SpeechRecognition || (window as any).webkitSpeechRecognition;\n\nlet recognition: SpeechRecognition | null;\n\nexport type ResultType = {\n  speechBlob?: Blob;\n  timestamp: number;\n  transcript: string;\n};\n\n// Set recognition back to null for brave browser due to promise resolving\n// after the conditional on line 31\nif ((navigator as BraveNavigator).brave) {\n  (navigator as BraveNavigator).brave.isBrave().then((bool) => {\n    if (bool) recognition = null;\n  });\n}\n\n// Chromium browsers will have the SpeechRecognition method\n// but do not implement the functionality due to google wanting ðŸ’°\n// this covers new Edge and line 22 covers Brave, the two most popular non-chrome chromium browsers\nif (!isEdgeChromium && SpeechRecognition) {\n  recognition = new SpeechRecognition();\n}\n\nexport interface UseSpeechToTextTypes {\n  continuous?: boolean;\n  crossBrowser?: boolean;\n  googleApiKey?: string;\n  googleCloudRecognitionConfig?: GoogleCloudRecognitionConfig;\n  onStartSpeaking?: () => any;\n  onStoppedSpeaking?: () => any;\n  speechRecognitionProperties?: SpeechRecognitionProperties;\n  timeout?: number;\n  useLegacyResults?: boolean;\n  useOnlyGoogleCloud?: boolean;\n}\n\nexport default function useSpeechToText({\n  continuous,\n  crossBrowser,\n  googleApiKey,\n  googleCloudRecognitionConfig,\n  onStartSpeaking,\n  onStoppedSpeaking,\n  speechRecognitionProperties = { interimResults: true },\n  timeout = 10000,\n  useOnlyGoogleCloud = false,\n  useLegacyResults = true\n}: UseSpeechToTextTypes) {\n  const [isRecording, setIsRecording] = useState(false);\n\n  const audioContextRef = useRef<AudioContext>();\n\n  const [legacyResults, setLegacyResults] = useState<string[]>([]);\n  const [results, setResults] = useState<ResultType[]>([]);\n  const [sentence, setSentence] = useState('');\n\n  const [interimResult, setInterimResult] = useState<string | undefined>();\n  const [error, setError] = useState('');\n\n  const timeoutId = useRef<number>();\n  const mediaStream = useRef<MediaStream>();\n\n  useEffect(() => {\n    if (!crossBrowser && !recognition) {\n      setError('Speech Recognition API is only available on Chrome');\n    }\n\n    if (!navigator?.mediaDevices?.getUserMedia) {\n      setError('getUserMedia is not supported on this device/browser :(');\n    }\n\n    if ((crossBrowser || useOnlyGoogleCloud) && !googleApiKey) {\n      console.error(\n        'No google cloud API key was passed, google API will not be able to process speech'\n      );\n    }\n\n    if (!audioContextRef.current) {\n      audioContextRef.current = new AudioContext();\n    }\n\n    if (useLegacyResults) {\n      console.warn(\n        'react-hook-speech-to-text is using legacy results, pass useLegacyResults: false to the hook to use the new array of objects results. Legacy array of strings results will be removed in a future version.'\n      );\n    }\n  }, []);\n\n  // Chrome Speech Recognition API:\n  // Only supported on Chrome browsers\n  const chromeSpeechRecognition = () => {\n    if (recognition) {\n      // Continuous recording after stopped speaking event\n      if (continuous) recognition.continuous = true;\n\n      const { grammars, interimResults, lang, maxAlternatives } =\n        speechRecognitionProperties || {};\n\n      if (grammars) recognition.grammars = grammars;\n      if (lang) recognition.lang = lang;\n\n      recognition.interimResults = interimResults || false;\n      recognition.maxAlternatives = maxAlternatives || 1;\n\n      // start recognition\n      recognition.start();\n\n      // speech successfully translated into text\n      recognition.onresult = (e) => {\n        const result = e.results[e.results.length - 1];\n        const { transcript } = result[0];\n        const timestamp = Math.floor(Date.now() / 1000);\n\n        // Allows for realtime speech result UI feedback\n        if (interimResults) {\n          if (result.isFinal) {\n            setInterimResult(undefined);\n            setResults((prevResults) => [\n              ...prevResults,\n              { transcript, timestamp }\n            ]);\n            setLegacyResults((prevResults) => [...prevResults, transcript]);\n            setSentence(transcript);\n            analizarMensaje(transcript);\n          } else {\n            let concatTranscripts = '';\n\n            // If continuous: e.results will include previous speech results: need to start loop at the current event resultIndex for proper concatenation\n            for (let i = e.resultIndex; i < e.results.length; i++) {\n              concatTranscripts += e.results[i][0].transcript;\n            }\n\n            setInterimResult(concatTranscripts);\n          }\n        } else {\n          setResults((prevResults) => [\n            ...prevResults,\n            { transcript, timestamp }\n          ]);\n          setLegacyResults((prevResults) => [...prevResults, transcript]);\n        }\n      };\n\n      recognition.onaudiostart = () => setIsRecording(true);\n\n      // Audio stopped recording or timed out.\n      // Chrome speech auto times-out if no speech after a while\n      recognition.onend = () => {\n        setIsRecording(false);\n      };\n    }\n  };\n\n  const startSpeechToText = async () => {\n    if (!useOnlyGoogleCloud && recognition) {\n      chromeSpeechRecognition();\n      return;\n    }\n\n    if (!crossBrowser && !useOnlyGoogleCloud) {\n      return;\n    }\n\n    // Resume audio context due to google auto play policy\n    // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio\n    if (audioContextRef.current?.state === 'suspended') {\n      audioContextRef.current?.resume();\n    }\n\n    const stream = await startRecording({\n      errHandler: () => setError('Microphone permission was denied'),\n      audioContext: audioContextRef.current as AudioContext\n    });\n\n    setIsRecording(true);\n\n    // Stop recording if timeout\n    if (timeout) {\n      clearTimeout(timeoutId.current);\n      handleRecordingTimeout();\n    }\n\n    // stop previous mediaStream track if exists\n    if (mediaStream.current) {\n      stopMediaStream();\n    }\n\n    // Clones stream to fix hark bug on Safari\n    mediaStream.current = stream.clone();\n\n    const speechEvents = Hark(mediaStream.current, {\n      audioContext: audioContextRef.current as AudioContext\n    });\n\n    speechEvents.on('speaking', () => {\n      if (onStartSpeaking) onStartSpeaking();\n\n      // Clear previous recording timeout on every speech event\n      clearTimeout(timeoutId.current);\n    });\n\n    speechEvents.on('stopped_speaking', () => {\n      if (onStoppedSpeaking) onStoppedSpeaking();\n\n      // Stops current recording and sends audio string to google cloud.\n      // recording will start again after google cloud api\n      // call if `continuous` prop is true. Until the api result\n      // returns, technically the microphone is not being captured again\n      stopRecording({\n        exportWAV: true,\n        wavCallback: (blob) =>\n          handleBlobToBase64({ blob, continuous: continuous || false })\n      });\n    });\n  };\n\n  const stopSpeechToText = () => {\n    if (recognition && !useOnlyGoogleCloud) {\n      recognition.stop();\n    } else {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({\n        exportWAV: true,\n        wavCallback: (blob) => handleBlobToBase64({ blob, continuous: false })\n      });\n    }\n  };\n\n  const handleRecordingTimeout = () => {\n    timeoutId.current = window.setTimeout(() => {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({ exportWAV: false });\n    }, timeout);\n  };\n\n  const handleBlobToBase64 = ({\n    blob,\n    continuous\n  }: {\n    blob: Blob;\n    continuous: boolean;\n  }) => {\n    const reader = new FileReader();\n    reader.readAsDataURL(blob);\n\n    reader.onloadend = async () => {\n      const base64data = reader.result as string;\n\n      let sampleRate = audioContextRef.current?.sampleRate;\n\n      // Google only accepts max 48000 sample rate: if\n      // greater recorder js will down-sample to 48000\n      if (sampleRate && sampleRate > 48000) {\n        sampleRate = 48000;\n      }\n\n      const audio = { content: '' };\n\n      const config: GoogleCloudRecognitionConfig = {\n        encoding: 'LINEAR16',\n        languageCode: 'es-ES',\n        sampleRateHertz: sampleRate,\n        ...googleCloudRecognitionConfig\n      };\n\n      const data = {\n        config,\n        audio\n      };\n\n      // Gets raw base 64 string data\n      audio.content = base64data.substr(base64data.indexOf(',') + 1);\n\n      const googleCloudRes = await fetch(\n        `https://speech.googleapis.com/v1/speech:recognize?key=${googleApiKey}`,\n        {\n          method: 'POST',\n          body: JSON.stringify(data)\n        }\n      );\n\n      const googleCloudJson = await googleCloudRes.json();\n\n      // Update results state with transcribed text\n      if (googleCloudJson.results?.length > 0) {\n        const { transcript } = googleCloudJson.results[0].alternatives[0];\n        setLegacyResults((prevResults) => [...prevResults, transcript]);\n\n        setResults((prevResults) => [\n          ...prevResults,\n          {\n            speechBlob: blob,\n            transcript,\n            timestamp: Math.floor(Date.now() / 1000)\n          }\n        ]);\n      }\n\n      if (continuous) {\n        startSpeechToText();\n      } else {\n        stopMediaStream();\n        setIsRecording(false);\n      }\n    };\n  };\n\n  const stopMediaStream = () => {\n    mediaStream.current?.getAudioTracks()[0].stop();\n  };\n\n  const analizarMensaje = (mensaje) =>{\n      //Consultar a dialogFlow\n      \n  };\n\n  return {\n    error,\n    interimResult,\n    isRecording,\n    sentence,\n    results: useLegacyResults ? legacyResults : results,\n    setResults,\n    startSpeechToText,\n    stopSpeechToText\n  };\n}\n"]},"metadata":{},"sourceType":"module"}