[{"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/index.js":"1","/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/App.tsx":"2","/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/index.tsx":"3","/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/recorderHelpers.js":"4","/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/recorder.js":"5"},{"size":193,"mtime":1636846749148,"results":"6","hashOfConfig":"7"},{"size":2772,"mtime":1638322578396,"results":"8","hashOfConfig":"7"},{"size":11718,"mtime":1638319910947,"results":"9","hashOfConfig":"7"},{"size":1332,"mtime":1630186283000,"results":"10","hashOfConfig":"7"},{"size":8484,"mtime":1630186283000,"results":"11","hashOfConfig":"7"},{"filePath":"12","messages":"13","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"14"},"vfqucz",{"filePath":"15","messages":"16","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"17","messages":"18","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"19","usedDeprecatedRules":"20"},{"filePath":"21","messages":"22","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"14"},{"filePath":"23","messages":"24","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"25","usedDeprecatedRules":"14"},"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/index.js",[],["26","27"],"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/App.tsx",["28","29","30"],"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/index.tsx",["31"],"import { useState, useEffect, useRef } from 'react';\nimport Hark from 'hark';\nimport { startRecording, stopRecording } from './recorderHelpers';\nimport { data } from \"../detallePictogramas.json\";\n\nimport { GoogleCloudRecognitionConfig } from './GoogleCloudRecognitionConfig';\nimport axios from \"axios\";\n\nconst server = \"http://localhost:5000\";\n\nexport interface SpeechRecognitionProperties {\n  // continuous: do not pass continuous here, instead pass it as a param to the hook\n  grammars?: SpeechGrammarList;\n  interimResults?: boolean;\n  lang?: string;\n  maxAlternatives?: number;\n}\n\nconst isEdgeChromium = navigator.userAgent.indexOf('Edg/') !== -1;\n\ninterface BraveNavigator extends Navigator {\n  brave: {\n    isBrave: () => Promise<boolean>;\n  };\n}\n\nconst AudioContext = window.AudioContext || (window as any).webkitAudioContext;\n\nconst SpeechRecognition =\n  window.SpeechRecognition || (window as any).webkitSpeechRecognition;\n\nlet recognition: SpeechRecognition | null;\n\nexport type ResultType = {\n  speechBlob?: Blob;\n  timestamp: number;\n  transcript: string;\n};\n\nexport type PictogramasList = {\n  image: string;\n  title: string;\n  desc: string;\n}\n\n// Set recognition back to null for brave browser due to promise resolving\n// after the conditional on line 31\nif ((navigator as BraveNavigator).brave) {\n  (navigator as BraveNavigator).brave.isBrave().then((bool) => {\n    if (bool) recognition = null;\n  });\n}\n\n// Chromium browsers will have the SpeechRecognition method\n// but do not implement the functionality due to google wanting ðŸ’°\n// this covers new Edge and line 22 covers Brave, the two most popular non-chrome chromium browsers\nif (!isEdgeChromium && SpeechRecognition) {\n  recognition = new SpeechRecognition();\n}\n\nexport interface UseSpeechToTextTypes {\n  continuous?: boolean;\n  crossBrowser?: boolean;\n  googleApiKey?: string;\n  googleCloudRecognitionConfig?: GoogleCloudRecognitionConfig;\n  onStartSpeaking?: () => any;\n  onStoppedSpeaking?: () => any;\n  speechRecognitionProperties?: SpeechRecognitionProperties;\n  timeout?: number;\n  useLegacyResults?: boolean;\n  useOnlyGoogleCloud?: boolean;\n}\n\nexport default function useSpeechToText({\n  continuous,\n  crossBrowser,\n  googleApiKey,\n  googleCloudRecognitionConfig,\n  onStartSpeaking,\n  onStoppedSpeaking,\n  speechRecognitionProperties = { interimResults: true },\n  timeout = 10000,\n  useOnlyGoogleCloud = false,\n  useLegacyResults = true\n}: UseSpeechToTextTypes) {\n  const [isRecording, setIsRecording] = useState(false);\n\n  const audioContextRef = useRef<AudioContext>();\n\n  const [legacyResults, setLegacyResults] = useState<string[]>([]);\n  const [results, setResults] = useState<ResultType[]>([]);\n  const [sentence, setSentence] = useState('');\n  const [isLoading, setLoading] = useState(false);\n  const [pictogramas, setPictogramas] = useState<PictogramasList[]>([]);\n  const [interimResult, setInterimResult] = useState<string | undefined>();\n  const [error, setError] = useState('');\n\n  const timeoutId = useRef<number>();\n  const mediaStream = useRef<MediaStream>();\n\n  useEffect(() => {\n    if (!crossBrowser && !recognition) {\n      setError('Speech Recognition API is only available on Chrome');\n    }\n\n    if (!navigator?.mediaDevices?.getUserMedia) {\n      setError('getUserMedia is not supported on this device/browser :(');\n    }\n\n    if ((crossBrowser || useOnlyGoogleCloud) && !googleApiKey) {\n      console.error(\n        'No google cloud API key was passed, google API will not be able to process speech'\n      );\n    }\n\n    if (!audioContextRef.current) {\n      audioContextRef.current = new AudioContext();\n    }\n\n    if (useLegacyResults) {\n      console.warn(\n        'react-hook-speech-to-text is using legacy results, pass useLegacyResults: false to the hook to use the new array of objects results. Legacy array of strings results will be removed in a future version.'\n      );\n    }\n  }, []);\n\n  // Chrome Speech Recognition API:\n  // Only supported on Chrome browsers\n  const chromeSpeechRecognition = () => {\n    if (recognition) {\n      // Continuous recording after stopped speaking event\n      if (continuous) recognition.continuous = true;\n\n      const { grammars, interimResults, lang, maxAlternatives } =\n        speechRecognitionProperties || {};\n\n      if (grammars) recognition.grammars = grammars;\n      if (lang) recognition.lang = lang;\n\n      recognition.interimResults = interimResults || false;\n      recognition.maxAlternatives = maxAlternatives || 1;\n\n      // start recognition\n      recognition.start();\n\n      // speech successfully translated into text\n      recognition.onresult = (e) => {\n        const result = e.results[e.results.length - 1];\n        const { transcript } = result[0];\n        const timestamp = Math.floor(Date.now() / 1000);\n\n        // Allows for realtime speech result UI feedback\n        if (interimResults) {\n          if (result.isFinal) {\n            setInterimResult(undefined);\n            setResults((prevResults) => [\n              ...prevResults,\n              { transcript, timestamp }\n            ]);\n            setLegacyResults((prevResults) => [...prevResults, transcript]);\n            analizarMensaje(transcript);\n          } else {\n            let concatTranscripts = '';\n\n            // If continuous: e.results will include previous speech results: need to start loop at the current event resultIndex for proper concatenation\n            for (let i = e.resultIndex; i < e.results.length; i++) {\n              concatTranscripts += e.results[i][0].transcript;\n            }\n\n            setInterimResult(concatTranscripts);\n          }\n        } else {\n          setResults((prevResults) => [\n            ...prevResults,\n            { transcript, timestamp }\n          ]);\n          setLegacyResults((prevResults) => [...prevResults, transcript]);\n        }\n      };\n\n      recognition.onaudiostart = () => setIsRecording(true);\n\n      // Audio stopped recording or timed out.\n      // Chrome speech auto times-out if no speech after a while\n      recognition.onend = () => {\n        setIsRecording(false);\n      };\n    }\n  };\n\n  const startSpeechToText = async () => {\n    if (!useOnlyGoogleCloud && recognition) {\n      chromeSpeechRecognition();\n      return;\n    }\n\n    if (!crossBrowser && !useOnlyGoogleCloud) {\n      return;\n    }\n\n    // Resume audio context due to google auto play policy\n    // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio\n    if (audioContextRef.current?.state === 'suspended') {\n      audioContextRef.current?.resume();\n    }\n\n    const stream = await startRecording({\n      errHandler: () => setError('Microphone permission was denied'),\n      audioContext: audioContextRef.current as AudioContext\n    });\n\n    setIsRecording(true);\n\n    // Stop recording if timeout\n    if (timeout) {\n      clearTimeout(timeoutId.current);\n      handleRecordingTimeout();\n    }\n\n    // stop previous mediaStream track if exists\n    if (mediaStream.current) {\n      stopMediaStream();\n    }\n\n    // Clones stream to fix hark bug on Safari\n    mediaStream.current = stream.clone();\n\n    const speechEvents = Hark(mediaStream.current, {\n      audioContext: audioContextRef.current as AudioContext\n    });\n\n    speechEvents.on('speaking', () => {\n      if (onStartSpeaking) onStartSpeaking();\n\n      // Clear previous recording timeout on every speech event\n      clearTimeout(timeoutId.current);\n    });\n\n    speechEvents.on('stopped_speaking', () => {\n      if (onStoppedSpeaking) onStoppedSpeaking();\n\n      // Stops current recording and sends audio string to google cloud.\n      // recording will start again after google cloud api\n      // call if `continuous` prop is true. Until the api result\n      // returns, technically the microphone is not being captured again\n      stopRecording({\n        exportWAV: true,\n        wavCallback: (blob) =>\n          handleBlobToBase64({ blob, continuous: continuous || false })\n      });\n    });\n  };\n\n  const stopSpeechToText = () => {\n    if (recognition && !useOnlyGoogleCloud) {\n      recognition.stop();\n    } else {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({\n        exportWAV: true,\n        wavCallback: (blob) => handleBlobToBase64({ blob, continuous: false })\n      });\n    }\n  };\n\n  const handleRecordingTimeout = () => {\n    timeoutId.current = window.setTimeout(() => {\n      setIsRecording(false);\n      stopMediaStream();\n      stopRecording({ exportWAV: false });\n    }, timeout);\n  };\n\n  const handleBlobToBase64 = ({\n    blob,\n    continuous\n  }: {\n    blob: Blob;\n    continuous: boolean;\n  }) => {\n    const reader = new FileReader();\n    reader.readAsDataURL(blob);\n\n    reader.onloadend = async () => {\n      const base64data = reader.result as string;\n\n      let sampleRate = audioContextRef.current?.sampleRate;\n\n      // Google only accepts max 48000 sample rate: if\n      // greater recorder js will down-sample to 48000\n      if (sampleRate && sampleRate > 48000) {\n        sampleRate = 48000;\n      }\n\n      const audio = { content: '' };\n\n      const config: GoogleCloudRecognitionConfig = {\n        encoding: 'LINEAR16',\n        languageCode: 'es-ES',\n        sampleRateHertz: sampleRate,\n        ...googleCloudRecognitionConfig\n      };\n\n      const data = {\n        config,\n        audio\n      };\n\n      // Gets raw base 64 string data\n      audio.content = base64data.substr(base64data.indexOf(',') + 1);\n\n      const googleCloudRes = await fetch(\n        `https://speech.googleapis.com/v1/speech:recognize?key=${googleApiKey}`,\n        {\n          method: 'POST',\n          body: JSON.stringify(data)\n        }\n      );\n\n      const googleCloudJson = await googleCloudRes.json();\n\n      // Update results state with transcribed text\n      if (googleCloudJson.results?.length > 0) {\n        const { transcript } = googleCloudJson.results[0].alternatives[0];\n        setLegacyResults((prevResults) => [...prevResults, transcript]);\n\n        setResults((prevResults) => [\n          ...prevResults,\n          {\n            speechBlob: blob,\n            transcript,\n            timestamp: Math.floor(Date.now() / 1000)\n          }\n        ]);\n      }\n\n      if (continuous) {\n        startSpeechToText();\n      } else {\n        stopMediaStream();\n        setIsRecording(false);\n      }\n    };\n  };\n\n  const stopMediaStream = () => {\n    mediaStream.current?.getAudioTracks()[0].stop();\n  };\n\n  const analizarMensaje = (mensaje: string) =>{\n      setSentence('');\n      if(mensaje.toLowerCase().includes('nico ')){\n        const query = mensaje.toLowerCase().split('nico ').pop() || '';\n        setSentence(query);\n        setLoading(true);\n        axios.post(`${server}/api/agent/text-input?message=${query}`, {})\n            .then(res => {\n              setLoading(false);\n              const intend = res.data.data[0]?.queryResult?.fulfillmentMessages[0]?.text?.text[0];\n              const result:any = data.filter(i => i.intent===intend);\n              if(result && result[0]?.opciones){\n                setPictogramas(result[0].opciones);\n              }else{\n                setPictogramas([]);\n              }\n        });\n      }\n  };\n\n  return {\n    error,\n    interimResult,\n    isRecording,\n    isLoading,\n    pictogramas,\n    sentence,\n    results: useLegacyResults ? legacyResults : results,\n    setResults,\n    startSpeechToText,\n    stopSpeechToText\n  };\n}\n\nconst msg = new SpeechSynthesisUtterance();\nconst voices = window.speechSynthesis.getVoices();\nmsg.voice = voices[2];\nmsg.volume = 1;\nmsg.rate = 10;\nmsg.pitch = 2;\nmsg.lang = 'es-MX';\n\nexport const reproducirMensaje=(item:any)=>{\n  if(item && item.desc){\n    msg.text = item.desc;\n    speechSynthesis.speak(msg);\n  }\n}\n",["32","33"],"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/recorderHelpers.js",[],"/Users/rodolfopavez/Desktop/ideaufro/frontend/frontend/src/Hooks/recorder.js",["34","35","36"],"import InlineWorker from 'inline-worker';\n\nexport class Recorder {\n  constructor(source, cfg) {\n    this.config = {\n      bufferLen: 4096,\n      numChannels: 1,\n      mimeType: 'audio/wav',\n      ...cfg\n    };\n    this.recording = false;\n    this.callbacks = {\n      getBuffer: [],\n      exportWAV: []\n    };\n    this.context = source.context;\n    this.node = (\n      this.context.createScriptProcessor || this.context.createJavaScriptNode\n    ).call(\n      this.context,\n      this.config.bufferLen,\n      this.config.numChannels,\n      this.config.numChannels\n    );\n\n    this.node.onaudioprocess = (e) => {\n      if (!this.recording) return;\n\n      var buffer = [];\n      for (var channel = 0; channel < this.config.numChannels; channel++) {\n        buffer.push(e.inputBuffer.getChannelData(channel));\n      }\n      this.worker.postMessage({\n        command: 'record',\n        buffer: buffer\n      });\n    };\n\n    source.connect(this.node);\n    this.node.connect(this.context.destination); //this should not be necessary\n\n    let self = {};\n    this.worker = new InlineWorker(function () {\n      let recLength = 0,\n        recBuffers = [],\n        sampleRate,\n        numChannels;\n\n      this.onmessage = function (e) {\n        switch (e.data.command) {\n          case 'init':\n            init(e.data.config);\n            break;\n          case 'record':\n            record(e.data.buffer);\n            break;\n          case 'exportWAV':\n            exportWAV(e.data.type);\n            break;\n          case 'getBuffer':\n            getBuffer();\n            break;\n          case 'clear':\n            clear();\n            break;\n        }\n      };\n\n      let newSampleRate;\n\n      function init(config) {\n        sampleRate = config.sampleRate;\n        numChannels = config.numChannels;\n        initBuffers();\n\n        if (sampleRate > 48000) {\n          newSampleRate = 48000;\n        } else {\n          newSampleRate = sampleRate;\n        }\n      }\n\n      function record(inputBuffer) {\n        for (var channel = 0; channel < numChannels; channel++) {\n          recBuffers[channel].push(inputBuffer[channel]);\n        }\n        recLength += inputBuffer[0].length;\n      }\n\n      function exportWAV(type) {\n        let buffers = [];\n        for (let channel = 0; channel < numChannels; channel++) {\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\n        }\n        let interleaved;\n        if (numChannels === 2) {\n          interleaved = interleave(buffers[0], buffers[1]);\n        } else {\n          interleaved = buffers[0];\n        }\n\n        // converts sample rate to 48000 if higher than 48000\n        let downSampledBuffer = downSampleBuffer(interleaved, newSampleRate);\n\n        let dataview = encodeWAV(downSampledBuffer);\n        let audioBlob = new Blob([dataview], { type: type });\n\n        this.postMessage({ command: 'exportWAV', data: audioBlob });\n      }\n\n      function getBuffer() {\n        let buffers = [];\n        for (let channel = 0; channel < numChannels; channel++) {\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\n        }\n        this.postMessage({ command: 'getBuffer', data: buffers });\n      }\n\n      function clear() {\n        recLength = 0;\n        recBuffers = [];\n        initBuffers();\n      }\n\n      function initBuffers() {\n        for (let channel = 0; channel < numChannels; channel++) {\n          recBuffers[channel] = [];\n        }\n      }\n\n      function mergeBuffers(recBuffers, recLength) {\n        let result = new Float32Array(recLength);\n        let offset = 0;\n        for (let i = 0; i < recBuffers.length; i++) {\n          result.set(recBuffers[i], offset);\n          offset += recBuffers[i].length;\n        }\n        return result;\n      }\n\n      function interleave(inputL, inputR) {\n        let length = inputL.length + inputR.length;\n        let result = new Float32Array(length);\n\n        let index = 0,\n          inputIndex = 0;\n\n        while (index < length) {\n          result[index++] = inputL[inputIndex];\n          result[index++] = inputR[inputIndex];\n          inputIndex++;\n        }\n        return result;\n      }\n\n      function floatTo16BitPCM(output, offset, input) {\n        for (let i = 0; i < input.length; i++, offset += 2) {\n          let s = Math.max(-1, Math.min(1, input[i]));\n          output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\n        }\n      }\n\n      function writeString(view, offset, string) {\n        for (let i = 0; i < string.length; i++) {\n          view.setUint8(offset + i, string.charCodeAt(i));\n        }\n      }\n\n      // Down sample buffer before WAV encoding\n      function downSampleBuffer(buffer, rate) {\n        if (rate == sampleRate) {\n          return buffer;\n        }\n        if (rate > sampleRate) {\n          throw 'downsampling rate show be smaller than original sample rate';\n        }\n        var sampleRateRatio = sampleRate / rate;\n        var newLength = Math.round(buffer.length / sampleRateRatio);\n        var result = new Float32Array(newLength);\n        var offsetResult = 0;\n        var offsetBuffer = 0;\n        while (offsetResult < result.length) {\n          var nextOffsetBuffer = Math.round(\n            (offsetResult + 1) * sampleRateRatio\n          );\n          // Use average value of skipped samples\n          var accum = 0,\n            count = 0;\n          for (\n            var i = offsetBuffer;\n            i < nextOffsetBuffer && i < buffer.length;\n            i++\n          ) {\n            accum += buffer[i];\n            count++;\n          }\n          result[offsetResult] = accum / count;\n          // Or you can simply get rid of the skipped samples:\n          // result[offsetResult] = buffer[nextOffsetBuffer];\n          offsetResult++;\n          offsetBuffer = nextOffsetBuffer;\n        }\n        return result;\n      }\n\n      function encodeWAV(samples) {\n        let buffer = new ArrayBuffer(44 + samples.length * 2);\n        let view = new DataView(buffer);\n\n        /* RIFF identifier */\n        writeString(view, 0, 'RIFF');\n        /* RIFF chunk length */\n        view.setUint32(4, 36 + samples.length * 2, true);\n        /* RIFF type */\n        writeString(view, 8, 'WAVE');\n        /* format chunk identifier */\n        writeString(view, 12, 'fmt ');\n        /* format chunk length */\n        view.setUint32(16, 16, true);\n        /* sample format (raw) */\n        view.setUint16(20, 1, true);\n        /* channel count */\n        view.setUint16(22, numChannels, true);\n        /* sample rate */\n        view.setUint32(24, newSampleRate, true);\n        /* byte rate (sample rate * block align) */\n        view.setUint32(28, newSampleRate * 4, true);\n        /* block align (channel count * bytes per sample) */\n        view.setUint16(32, numChannels * 2, true);\n        /* bits per sample */\n        view.setUint16(34, 16, true);\n        /* data chunk identifier */\n        writeString(view, 36, 'data');\n        /* data chunk length */\n        view.setUint32(40, samples.length * 2, true);\n\n        floatTo16BitPCM(view, 44, samples);\n\n        return view;\n      }\n    }, self);\n\n    this.worker.postMessage({\n      command: 'init',\n      config: {\n        sampleRate: this.context.sampleRate,\n        numChannels: this.config.numChannels\n      }\n    });\n\n    this.worker.onmessage = (e) => {\n      let cb = this.callbacks[e.data.command].pop();\n      if (typeof cb == 'function') {\n        cb(e.data.data);\n      }\n    };\n  }\n\n  record() {\n    this.recording = true;\n  }\n\n  stop() {\n    this.recording = false;\n  }\n\n  clear() {\n    this.worker.postMessage({ command: 'clear' });\n  }\n\n  getBuffer(cb) {\n    cb = cb || this.config.callback;\n    if (!cb) throw new Error('Callback not set');\n\n    this.callbacks.getBuffer.push(cb);\n\n    this.worker.postMessage({ command: 'getBuffer' });\n  }\n\n  exportWAV(cb, mimeType) {\n    mimeType = mimeType || this.config.mimeType;\n    cb = cb || this.config.callback;\n    if (!cb) throw new Error('Callback not set');\n\n    this.callbacks.exportWAV.push(cb);\n\n    this.worker.postMessage({\n      command: 'exportWAV',\n      type: mimeType\n    });\n  }\n\n  static forceDownload(blob, filename) {\n    let url = (window.URL || window.webkitURL).createObjectURL(blob);\n    let link = window.document.createElement('a');\n    link.href = url;\n    link.download = filename || 'output.wav';\n    let click = document.createEvent('Event');\n    click.initEvent('click', true, true);\n    link.dispatchEvent(click);\n  }\n}\n\nexport default Recorder;\n",{"ruleId":"37","replacedBy":"38"},{"ruleId":"39","replacedBy":"40"},{"ruleId":"41","severity":1,"message":"42","line":4,"column":27,"nodeType":"43","messageId":"44","endLine":4,"endColumn":37},{"ruleId":"41","severity":1,"message":"45","line":14,"column":5,"nodeType":"43","messageId":"44","endLine":14,"endColumn":18},{"ruleId":"41","severity":1,"message":"46","line":19,"column":5,"nodeType":"43","messageId":"44","endLine":19,"endColumn":12},{"ruleId":"47","severity":1,"message":"48","line":125,"column":6,"nodeType":"49","endLine":125,"endColumn":8,"suggestions":"50"},{"ruleId":"37","replacedBy":"38"},{"ruleId":"39","replacedBy":"40"},{"ruleId":"51","severity":1,"message":"52","line":50,"column":9,"nodeType":"53","messageId":"54","endLine":66,"endColumn":10},{"ruleId":"55","severity":1,"message":"56","line":171,"column":18,"nodeType":"57","messageId":"58","endLine":171,"endColumn":20},{"ruleId":"59","severity":1,"message":"60","line":175,"column":11,"nodeType":"61","messageId":"62","endLine":175,"endColumn":79},"no-native-reassign",["63"],"no-negated-in-lhs",["64"],"@typescript-eslint/no-unused-vars","'ResultType' is defined but never used.","Identifier","unusedVar","'interimResult' is assigned a value but never used.","'results' is assigned a value but never used.","react-hooks/exhaustive-deps","React Hook useEffect has missing dependencies: 'crossBrowser', 'googleApiKey', 'useLegacyResults', and 'useOnlyGoogleCloud'. Either include them or remove the dependency array.","ArrayExpression",["65"],"default-case","Expected a default case.","SwitchStatement","missingDefaultCase","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected","no-throw-literal","Expected an error object to be thrown.","ThrowStatement","object","no-global-assign","no-unsafe-negation",{"desc":"66","fix":"67"},"Update the dependencies array to be: [crossBrowser, googleApiKey, useLegacyResults, useOnlyGoogleCloud]",{"range":"68","text":"69"},[3893,3895],"[crossBrowser, googleApiKey, useLegacyResults, useOnlyGoogleCloud]"]